# codeGen
Generate code using a local small model like llama3.2 3b. Write tests first then use tests to verify that the code works.

## architecture
LLM - local llm running on current system or another system over network.
Code sandbox - docker container to have the llm run live code.